METHOD_0 ( ) { org . apache . hadoop . IDENT_0 . IDENT_1 job = new org . apache . hadoop . IDENT_0 . METHOD_1 ( ) ; job . set ( IDENT_2 . IDENT_3 , org . apache . hadoop . IDENT_0 . IDENT_4 . IDENT_5 ) ; job . set ( org . apache . hadoop . mapreduce . lib . output . IDENT_6 . IDENT_7 , STRING_0 ) ; org . apache . hadoop . IDENT_0 . IDENT_6 . METHOD_2 ( job , org . apache . hadoop . IDENT_0 . IDENT_4 . IDENT_8 . METHOD_3 ( ) . METHOD_3 ( ) ) ; org . apache . hadoop . IDENT_0 . IDENT_6 . METHOD_4 ( job , org . apache . hadoop . IDENT_0 . IDENT_4 . IDENT_8 ) ; org . apache . hadoop . fs . IDENT_9 fs = org . apache . hadoop . IDENT_0 . IDENT_4 . IDENT_8 . METHOD_5 ( job ) ; if ( ! ( fs . METHOD_6 ( org . apache . hadoop . IDENT_0 . IDENT_4 . IDENT_8 ) ) ) { org . junit . Assert . fail ( STRING_1 ) ; } java . lang . String file = STRING_2 ; org . apache . hadoop . IDENT_0 . IDENT_10 IDENT_11 = IDENT_10 . IDENT_12 ; org . apache . hadoop . IDENT_0 . IDENT_13 < java . lang . Object , java . lang . Object > IDENT_14 = new org . apache . hadoop . IDENT_0 . IDENT_13 < java . lang . Object , java . lang . Object > ( ) ; org . apache . hadoop . IDENT_0 . IDENT_15 < java . lang . Object , java . lang . Object > IDENT_16 = IDENT_14 . METHOD_7 ( org . apache . hadoop . IDENT_0 . IDENT_4 . IDENT_17 , job , file , IDENT_11 ) ; org . apache . hadoop . io . IDENT_18 IDENT_19 = new org . apache . hadoop . io . METHOD_8 ( STRING_3 ) ; org . apache . hadoop . io . METHOD_8 IDENT_20 = new org . apache . hadoop . io . METHOD_8 ( STRING_4 ) ; org . apache . hadoop . io . METHOD_8 IDENT_21 = new org . apache . hadoop . io . METHOD_8 ( STRING_5 ) ; org . apache . hadoop . io . METHOD_8 IDENT_22 = new org . apache . hadoop . io . METHOD_8 ( STRING_6 ) ; org . apache . hadoop . io . IDENT_23 IDENT_24 = org . apache . hadoop . io . IDENT_23 . get ( ) ; try { IDENT_16 . write ( IDENT_19 , IDENT_21 ) ; IDENT_16 . write ( null , IDENT_24 ) ; IDENT_16 . write ( null , IDENT_21 ) ; IDENT_16 . write ( IDENT_24 , IDENT_22 ) ; IDENT_16 . write ( IDENT_20 , IDENT_24 ) ; IDENT_16 . write ( IDENT_19 , null ) ; IDENT_16 . write ( null , null ) ; IDENT_16 . write ( IDENT_20 , IDENT_22 ) ; } finally { IDENT_16 . close ( IDENT_11 ) ; } java . lang . StringBuffer IDENT_25 = new java . lang . StringBuffer ( ) ; IDENT_25 . append ( IDENT_19 ) . append ( STRING_7 ) . append ( IDENT_21 ) . append ( "\n" ) ; IDENT_25 . append ( IDENT_21 ) . append ( "\n" ) ; IDENT_25 . append ( IDENT_22 ) . append ( "\n" ) ; IDENT_25 . append ( IDENT_20 ) . append ( "\n" ) ; IDENT_25 . append ( IDENT_19 ) . append ( "\n" ) ; IDENT_25 . append ( IDENT_20 ) . append ( STRING_7 ) . append ( IDENT_22 ) . append ( "\n" ) ; org . apache . hadoop . io . IDENT_26 . IDENT_27 codec = new org . apache . hadoop . io . IDENT_26 . METHOD_9 ( ) ; codec . METHOD_10 ( job ) ; org . apache . hadoop . fs . Path IDENT_28 = new org . apache . hadoop . fs . Path ( org . apache . hadoop . IDENT_0 . IDENT_4 . IDENT_8 , ( file + ( codec . METHOD_11 ( ) ) ) ) ; final java . io . IDENT_29 IDENT_30 = new java . io . METHOD_12 ( IDENT_28 . toString ( ) ) ; org . apache . hadoop . io . IDENT_26 . IDENT_31 IDENT_32 = codec . METHOD_13 ( IDENT_30 ) ; org . apache . hadoop . util . IDENT_33 reader = new org . apache . hadoop . util . METHOD_14 ( IDENT_32 ) ; java . lang . String output = "" ; org . apache . hadoop . io . METHOD_8 out = new org . apache . hadoop . io . METHOD_8 ( ) ; while ( ( reader . METHOD_15 ( out ) ) > 0 ) { output += out ; output += "\n" ; } reader . close ( ) ; "<AssertPlaceHolder>" ; } toString ( ) { if ( ( json ) == null ) { return STRING_8 + ( id ) ; } else { return json . toString ( ) ; } }
org . junit . Assert . assertEquals ( IDENT_25 . toString ( ) , output ) 